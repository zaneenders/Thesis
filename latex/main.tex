\documentclass{article}

% Language setting
% Replace `english' with e.g. `spanish' to change the document language
\usepackage[english]{babel}

% Set page size and margins
% Replace `letterpaper' with `a4paper' for UK/EU standard size
\usepackage[letterpaper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

% Useful packages
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{setspace}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}

\title{Fitting Regimes into L1 cache}
\author{Zane Enders}

\doublespacing
\begin{document}
\maketitle

\section{Abstract}

Why your data layout can have a big impact on the performance of your programs? In today's world of Docker containers, JavaScipt on the server all being remotely authored from your your new ARM computer it's easy to forget that software does have a cost. Here I present an example of why the data layout and being mindful of heap access can really pay off. In doing so we were able to fit a dynamic optimization algorithm into L1 cache nearly tripling the performance of the algorithm and shaving 10 minutes off of a 14-minute benchmark runtime for the Herbie Floating point compiler.

Herbie is what we call a floating point compiler which means it takes mathematical expressions like $(x + 1) - x$ and rewrites them to an alternative expression that is more accurate on specified inputs or more per-formant than the naive implementation. Herbie has many of what I like to think of as phases as it goes through this work of finding alternative expressions. Sampling, setting up, and pre-processing the inputs before they are sent to the e-graph. Then extract the various alternative expressions from the e-graph. The part that I optimized for my thesis is a procedure called Regimes which is a search algorithm used to combine multiple alternatives (alt) to potentially create a new alt that is better than its descendants using branching. Branching does have a cost that can vary a lot depending on where the code is being run. But it’s often worth seeing what possible solutions can be from bringing two expressions together. The algorithm that performs this search is dependent on two input sizes the number of points that it is sampling from and the number of alternative expressions. Because these are codependent for determining the output this means that the runtime of this algorithm depends on the size of those inputs. I also go into considering where and when to use memory. As writing and reading from main memory will almost always be more expensive than most computations a CPU can perform.
\section{Regimes Algorithm}

\subsection{What is Regimes?}

Regimes take a list of alternatives in the form of error for each of the test points (The algorithm uses a predefined constant of 256 points or the training set of the 8000 sampled points.) and associated split indexes which decide whether we can split or not and outputs a new list of alternatives that may have some notion of branching between the two or more alternatives. How do we write an algorithm to execute this high-level idea? Well here is how the original version worked before we did some heavy modifications to make it go 3.5x faster.

\subsection{Original Algorithm}

The original regimes algorithm can be broken up into three parts: the initial setup, the core loop, and extraction. We first calculate a partial sum for each of the alts. This gives us an accumulated error as we move from left to right of the points. Which serves as a cumulative distribution of the error for a given alt if we use the alt as is up to that point. We use this partial sum to generate an initial list of candidates. The initial setup sets up an initial list of alts to iterate on later by creating a candidate for each point we pick the alt that has the lowest cumulative error for that point. This gives us a list of candidates who each have the lowest cumulative error for a given point or phrased another way the list of candidates who represent the best alternative up to that given point. Only the last candidate is a valid output after this initial setup as we can’t just ignore X number of points and say that’s the result you should use. If we have 256 points of error to split on we have to return an alt that covers all 256 points.

Now that we have our initial data we can move into the core loop of the algorithm. Which takes in this list of candidates and what we call split points until there are no more split points to be added. So we do one extra interaction of the loop to start as we don’t know if we are done until we see that there are no changes from consecutive calls to `add-splitpoint` giving us a base case for this recursive algorithm.

Within $add-splitpoint$ which takes in our list of candidates, we go over each of these candidates. We are attempting to add split points before the point to reduce our cumulative error after doing so. The first point can’t add any split points as it has no points to the left of it but it serves as a reference to know that at the first point, this is the best we can do. As this is a dynamic programming algorithm we are going to use these invalid candidates as a means of storing information and optimizing the sub-problems. As if we were to do this the naive way we would be $O((AP)^2)$ As for each point we would have to compare it against that point in all of the alts. Because of this, we are slowly building a list of ideal split points for a given sub-problem. As we loop over these candidates we first keep track of the current cost of the candidate. We save the current total as the cost of the current candidate minus a penalty to avoid the algorithm overfitting to just splitting on every point and just returning our initial list. Which may make sense except that branching has a cost that we need to account for. For this penalty, we are currently using the number of points in an alt which is arbitrary and some further research could be done in this area. An idea off the top of my head is this is proportional to the number of points between the split points. Favoring large gaps between split points. This could also be domain-specific as maybe you know that 90\% of your inputs will be between -1 and 1 well you want to penalize splits in that range more so that splits outside that range as if we are passing 10,000 points through this function we want the cached version of the computation to be the version that is between -1 and 1 and not branch so that this computation is very fast and statistically the most used code but we if we are getting high error outside the domain of -1 and 1 then we don’t want the accuracy of those points to suffer so the cost of swapping out the function in the CPU registers to a more accurate but slower version could be better. Also if you knew that inputs of -1 and 1 are going to be most of the inputs and the fastest part of your function and you have the luxury of being able to order the points beforehand you could pass in all points -1 to 1 first and then pass in the remaining points. This could be done using the `ucomisd` x86 instruction. On the points and bucket sorting based on if the values are slow vs fast.

Now that we have computed an adjusted cost of the current candidate we are optimizing we work through checking each of the preceding points up to the split point that this candidate represents. For each of these preceding points moving from left to right we check if we can split on this point which in most cases is true and I will be ignoring this detail for the rest of the paper as it is an edges case associated with certain alts and not core to the algorithm other then there are situations when we can and can’t make a split point. If we can potentially split on this point we need to compare its cumulative error to all other options for the point we are working on to find the lowest cumulative error for that region. We do this by calculating the difference between the cumulative error of the point we are iterating on and the cumulative error of the candidate we are currently optimizing. We save this difference and an index to which candidate has the current best-split point for this point. For differences in this sub-problem, we check if we are the local minimum for the current point we are optimizing and if we update the best-split point cost and index of which candidate we are using for this point. After we have checked against all the candidates for the point we are optimizing we know which candidate has the best partial sum difference for this point and what that difference is. We then check if the cumulative sum of the error before this new point plus the cost of the best candidate for this point is less than the cost current of the candidate we are optimizing less the penalty of creating the split point. We save this candidate and its split points plus this new split point as the current best candidate for this sub-problem, Which is the best candidate up to a point.  

That was a lot and maybe a good example of code being a better explanation of something than written English and maybe even Math if we look at the original Herbie paper Figure 6 Though these all have there own level of information density or terseness which makes some variants easier to digest than others depending on your background. 

So at the end of this $add-splitpoint$ function, we have a new list of candidates each representing a given sub-range of the problem which if possible added a split point lowering its cumulative error for that sub-problem. With only the last sub-problem being a valid output as it is the only subproblem that covers the range of points. The values are then compared against the input in the outer recursive loop/ function and if there is no difference after attempting to add a split point we have found the best set of split points we can find for the given set of alternatives. Because of how the sub-problems are set up and we are appending split points for a given candidate we need to reverse this list of split points for the last alt as the list of split points.


Now this algorithm and the implementation are great as it correctly does as it says of finding the optimal set of split points by dividing the space up into sub-problems and then solving each of those recursively. But as it stands it isn’t the fastest as you may have noticed we are looping over the number of points two times and then within that looping over the number of alts we are analyzing. So if we are passed in ten alternatives to optimize with we are doing up to 256 * 256 * 10 iterations of the inner loop which is performing a heap allocation in the form of a call to `cons` which is roughly $200-300 ns$ seconds which is roughly 0.13 to 0.19 seconds as a worst case if we manage to do every iteration of every loop. This may not sound like a lot of time but this adds up. Using the current version of Herbie at the time of writing this and the original algorithm this accounts for 14 minutes of the ~100 minutes that the nightly benchmark suite takes. As you will see later we can do much better.

\subsection{New Algorithm}

Because allocation is more expensive than most instructions the CPU can perform, especially any of the operations we perform in this algorithm. To remove this allocation the general thing to do is pre-allocate the memory that we plan to use. There are very few programs in this world that don’t use heap as you quickly run out of stack space and limit the kinds of problems you can solve or at least the way you can solve problems. So the next based thing you can do is be mindful about when you allocate and de-allocate from it. 
Racket is derived from a category of languages known as Lisp languages which are some of the first languages used to work on AI as they are very flexible in their basic form treating everything as a list that you can store and read from. This allows you to dynamically change how you do things based on where you are in the program. This sounds weird saying it out loud but in contrast to languages like C for which the goal at a high level was to have something easier to program in then assembly but still map the way a machine handles which does not have this level of dynamism that Lisp has as C doesn’t have the idea of a dynamic length list you have to build that yourself and if you ever built a linked list you should know that it is in its nieve form a randomly allocated points across the heap which is very flexible but when you think about caching and how a CPU works is pretty much disaster for making anything go fast. Engineering is all about trade offs as I am told and slowly learning and when to trade off dynamic flexibility vs aligning with the patterns that the CPU was designed to chug through is not always straightforward and very much depends on the program. Given the medium of programming has so much creative flexibility in what you can create most programmers prefer to play or work out ideas in the dynamic realm working to solve a problem for themselves or a collection of other people. Getting into the details of getting a CPU to perform a certain task fast is fun for some but a lot to keep in your head when you're working on solving a dynamic problem. This can be done we had assembly before we had Lisp we just have a lot of luxuries that we don’t always fully appreciate programming in today's world of abstractions.

Because Racket is a Lisp language but with the hindsight of lessons learned we don’t have to use a linked list for every operation and in fact the current version of this algorithm uses what's called a vector which is a fixed length array of elements. Another detail that we will touch on later that I learned well working on this is that this isn’t always a vector of things but can often be a vector of pointers to other things. This is another problem in allocation. If you are going to allocate a fixed region of memory for something you have to have some idea of how big the things are. The dumbest thing you can do is assume the biggest size of everything and have wasted space. Ideal the compiler knows how big the elements are or can infer from a type system what the elements will be and maybe have some insight into how much memory those things take up. 

So the first step is pre-allocating our output list of candidates and making some upper bound on the number of splits a candidate can have which is the number of points an alt as as that would mean we split on every point which in practice won’t happen because of our penalty that we apply for splitting. This is an improvement and allows us to remove the call to `cons` which is a heap allocation but doesn’t really solve the problem but a step in the right direction because we now have a fixed region of memory for what we need but not a fixed size for the things inside that region of memory. This is another pointer chase which is less bad than allocation because of pre-fetching but still not great when trying to go fast and if I remember correctly didn’t net us any gains in performance.

The next step is to take a look at what we are trying to store in this output array of candidates. Well, what is a candidate? It is a floating point value of the partial sum of the error for the region it represents and a list of split points for that candidate. We could assume the worst again here and make this list of split points a vector of split points with a length of the number of points we are working with. And for a moment let's do that, to ask another question: what is a split point in terms of memory? Well, a split point is two integers, an index to which candidate to use and which point to use that candidate for. These are effectively pointers but we won’t be chasing them here just creating them. So let's take a step back and lay out what we are working with. We have a vector of candidates for the number of points we are working with. A candidate is a floating point value and a vector of potential split points of the length of the number of points we are working with each containing two integers. So statically we might need 16 bytes for each split point. For the current version of Herbie we will always have 256 points so we will need (16 * 256) + 8 for the cumulative error giving us 4,104 bytes per candidate and we will have 256 candidates which brings us to a total of 1,050,624 bytes or a little over 1 mb of memory needed per call to add a split point. Which doesn’t fit into most L1 caches unless you have an ARM-based Mac. Which isn’t most computers so we are going to be going out to L2 well looping over this data which isn’t great.

Now this actually contains a bunch of redundant information as it is a flattened out Directed acyclic graph. This means we can reduce the size and fit into L1 which gives us a double win as we are spending less time moving memory and computing less redundant information. Using a simple example to help visualize this and looking at the output of the last iteration we can see that we have five copies of the same node. And 3 copies of another node and this example only has five points and three alternatives so as you can imagine this will have lots of redundant information for real inputs.

\begin{verbatim}
#(
 #(struct:cse 0 (#s(si 1 1)))
 #(struct:cse 18 (#s(si 2 2) #s(si 1 1)))
 #(struct:cse 65 (#s(si 2 3) #s(si 1 1)))
 #(struct:cse 131 (#s(si 0 4) #s(si 2 2) #s(si 1 1)))
 #(struct:cse 551 (#s(si 0 5) #s(si 2 2) #s(si 1 1))))
\end{verbatim}

This might be a little hard to read at first so elaborate. What this data is saying. The last list is actually in reverse which is common in dynamic programming problems as working backwards you build up to the answer. So if we flip that around we get the following list of split points.

\begin{verbatim}
(#s(si 1 1) #s(si 2 2) #s(si 0 5))
\end{verbatim}
This says that we should use the alt at index 1 up to point 1, then we should use alt two up to point two, and finally use alt zero for the remaining point which is point 5. This was very confusing to me at first as I had to switch between based 1 indexing and 0-based indexing. 

So how can we do better? Well, I think looking at the output gives us a good hint and shows us what the actual information is. We have 3 categories of numbers that we are working with: the cumulative error, the candidate index, and the point index. So maybe we can reorganize the data to reduce redundant information. If we flip this data structure 90 degrees and use the index of this 2D array as the point location we reduce this down to three arrays of 256 or the number of points. One of these arrays will represent the cumulative error for each point. One will be the alt which we use for that point and the third will be an index to which the alt is next in the output. Now there is some redundant information here and maybe we could do better but what we have really achieved is going from. Well, we have 3 arrays of length the number of points we are working with or 256 so we went from ~1 mb to ~768 bytes so we have made it into the L1 cache.
Now to actually work with these three arrays we need to do some bookkeeping for intermediate data, at least in this current implementation. I remember Pavel mentioning something about redundant information, but given the time constraint I had I think we did pretty well and at least improved the overall performance of Herbie a little bit, and I learned a lot about how to better organize data layouts. Future work was talking about dropping this all the way down to assembly but we decided that that might not be worth the complexity it would introduce given it is still only a fraction of the total runtime of Herbie and those engineering hours could be better spent on improving other parts of the project.
Now that I have described a little bit of the new data layout that we are working with. Let me go over what this new algorithm is that lets us shave 10 minutes off of a 14-minute runtime. This version of the algorithm doesn’t have as clear distinct phases as the original one did so the mental organization of this solution might be a little harder to read. But as I have learned the lower the level the code gets the closer it starts to look like C and then assembly as we have to map the information to this lower level somehow and some problems like this can’t be automatically synthesized from the compiler so we have to do it manually. As I mentioned before we are going to pre-allocate as much of the information as we can. Fortunately, this is just 3 arrays. One of these arrays is special as it is what Racket calls and `flvector` as Racket by default stores pointers to floating point numbers which is nice in most cases because you can change the number representation and not complicate the question of how do I store this thing at run time. `flvector` is an optimized vector so not only is it a fixed length of memory it also only stores floating point numbers. This adds some ceremony to the syntax needed to work with it but removes a pointer chase when working with the data. 

These 3 vectors are initially set to contain positive infinity, zeros, and the number of points. Positive infinite as this is our cumulative error for each point which is easy to logically improve on. We are storing zero in the alt index array to start though this is immediately replaced with alts but in the best/worst case we are just going to use the first alt for all of the points and not split at all. The last array says that we should use this alt up to the last point which as a sort of base case says that we should just use the first alt for all 256 points. This is effectively the same setup as initially in our previous algorithm.

Now that we have our base case setup we can work to optimize our error for each range of points. We start at the leftmost point and work in two phases. In the first part, we loop over all the candidates and determine which candidate has the smallest increase in error to the total sum for the current point. The second phase loops over each point in our output vectors and checks if the result of our sub-problem computed in the first phase is better than what we currently have including the penalty. And if we should add a new split point from this sub-problem. If the cost of using the new split point for our current point is less than what we have we go ahead and add a split point. In this second phase, there are a few ties in which we need to break in order to maintain a deterministic output. If the cost of adding the new alt is the same as our current error we arbitrarily favor the alt with the lower index or the first alt in the list of alts. We could maybe sort these by overall speed or cost to help aid this tie-breaking in the right direction. If we are comparing the same alt for both what we are currently using for a point and as a better alternative then we check which alt is used for the previous point and again favor the alt order first in our list of inputs.

At the end of this loop. We arrive at the same information as the previous algorithm though we have to do one last loop over the arrays to construct the list of split points in the format that the next function accepts. 


\subsection{Conclusion}

Overall, I learned a lot from this admittedly painful rewrite of this hindsight pretty standard dynamic programming problem. I can’t help but wonder how this might be better as I don’t fully feel satisfied with this rewrite but it did achieve the end goal of making the regimes algorithm faster as well as opening future work of exploring working with more alts. As the number of alts is now not in the inner loop expressions like the det 44 benchmark in Herbie which is computing the determinant of a 4x4 matrix. This is the benchmark in Herbie which has the most variables which correlates to having more alts to analyze as each variable is a sort of degree of freedom for Herbie to optimize over. Which went from spending 1.8 seconds down to 0.2 seconds. For expressions with lots of alts like det44, Herbie calls regimes on a few permutations. As I am writing up this analysis of the regimes algorithm I can’t help but have more questions about what could be better, and what other algorithms could work here. Going back over it and even rewriting both of the algorithms in my favorite language just to refresh my brain on the train of thought from 6 months ago. But I guess this is just a hard fact of life if you can’t do every optimization or make everything perfect like you can dream of in your head only learn from your mistakes and do better on the next problem striving to increase the quality of your output for others to iterate on or consume.

\href{https://dl.acm.org/doi/10.1145/2813885.2737959}{Automatically improving accuracy for floating point expressions}

\end{document}